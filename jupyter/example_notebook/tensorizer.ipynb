{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows the running of Coreweave's Tensorizer pip package for quickly loading models. This example pulls the model from a s3 endpoint hosted on Coreweave's servers, but you can also use your own endpoints or even pull the models locally.\n",
    "\n",
    "https://docs.coreweave.com/coreweave-machine-learning-and-ai/inference/tensorizer\n",
    "\n",
    "https://pypi.org/project/tensorizer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel after running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install tensorizer\n",
    "%pip install transformers\n",
    "%pip install ipywidgets widgetsnbextension pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from tensorizer import TensorDeserializer\n",
    "from tensorizer.utils import no_init_or_tensor, convert_bytes, get_mem_usage\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "#model_ref = \"EleutherAI/gpt-j-6B\"\n",
    "# To run this at home, swap this with the line below for a smaller example:\n",
    "model_ref = \"EleutherAI/gpt-neo-125M\"\n",
    "model_name = model_ref.split(\"/\")[-1]\n",
    "# Change this to your S3 bucket.\n",
    "s3_bucket = \"tensorized\"\n",
    "s3_uri = f\"s3://{s3_bucket}/{model_ref}/model.tensors\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures that the model is not initialized.\n",
    "with no_init_or_tensor():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "before_mem = get_mem_usage()\n",
    "\n",
    "# Lazy load the tensors from S3 into the model.\n",
    "start = time.time()\n",
    "deserializer = TensorDeserializer(s3_uri, plaid_mode=True)\n",
    "deserializer.load_into_module(model)\n",
    "end = time.time()\n",
    "\n",
    "# Brag about how fast we are.\n",
    "total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n",
    "duration = end - start\n",
    "per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n",
    "after_mem = get_mem_usage()\n",
    "deserializer.close()\n",
    "print(f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\")\n",
    "print(f\"Memory usage before: {before_mem}\")\n",
    "print(f\"Memory usage after: {after_mem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and generate\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ref)\n",
    "eos = tokenizer.eos_token_id\n",
    "input_ids = tokenizer.encode(\n",
    "    \"Â¡Hola! Encantado de conocerte. hoy voy a\", return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids, max_new_tokens=50, do_sample=True, pad_token_id=eos\n",
    "    )\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
